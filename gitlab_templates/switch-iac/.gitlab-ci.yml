default: 
  image: 
    name: ghcr.io/mojaloop/iac-aws-platform:1.3.2
  before_script:
    - "which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )"
    - export BUCKET=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - export TF_VAR_bucket=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - aws s3 sync s3://$BUCKET/bootstrap ./k8ss3bootstrap/ --sse || true
    - export CI_IMAGE_PROJECT_DIR=/iac-run-dir
    # Define the contents of the workbench-config file as environment variables for use in gitlab and optionally in terraform
    - for var in $(jq -r 'to_entries[] | "\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - for var in $(jq -r 'to_entries[] | "TF_VAR_\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - export ENVIRONMENT=$environment
    - test -n "$BUCKET"
    - cd $CI_IMAGE_PROJECT_DIR
    - aws s3 sync s3://${BUCKET}/${environment}/ . --sse || true
    - eval `ssh-agent`
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - test -f terraform/ssh_provisioner_key &&  chmod 0600 terraform/ssh_provisioner_key &&  ssh-add terraform/ssh_provisioner_key
    - chmod +x ./searchandreplace.sh
    - if [ -d ${CI_PROJECT_DIR}/iac-sync-dir ]; then cp -r ${CI_PROJECT_DIR}/iac-sync-dir/. .; else echo "no local files to copy"; fi 
    - cp $CI_PROJECT_DIR/workbench-config.json .
    - ./searchandreplace.sh
    
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - export ANSIBLE_FEATURE_TOGGLES="-e kube_version=v${k8s_api_version} -e dashboard_enabled=True -e kube_proxy_mode=iptables -e artifacts_dir=inventory/artifacts/cluster -e cloud_provider=aws -e ansible_user=ubuntu -b --become-user=root --flush-cache -e kubeconfig_localhost=true"

variables:
  #ANSIBLE_FEATURE_TOGGLES: "-e kube_version=v${k8s_api_version} -e dashboard_enabled=True -e kube_proxy_mode=iptables -e artifacts_dir=inventory/artifacts/cluster -e cloud_provider=aws -e ansible_user=ubuntu -b --become-user=root --flush-cache -e kubeconfig_localhost=true"
  GIT_SSL_NO_VERIFY: "true"

stages:
  - Validate
  - Destroy
  - "Deploy AWS infrastructure"
  - "Run Kubespray"
  - "Run k3s"
  - "Deploy Vault"
  - "Deploy Platform"
  - "Run Tests"
  - "Maintain Platform"

Validate and Plan:
  stage: Validate
  script:
    - cd terraform
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform plan -out=plan.cache
    - terraform show -json plan.cache | jq -r '([.resource_changes[]?.change.actions?]|flatten)|{"create":(map(select(.=="create"))|length),"update":(map(select(.=="update"))|length),"delete":(map(select(.=="delete"))|length)}' > ${CI_PROJECT_DIR}/plan.json
  artifacts:
    reports:
      terraform: plan.json
  when: manual

Destroy AWS:
  stage: Destroy
  script:
    - cd terraform
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve
  when: manual

Destroy Mojaloop K8s Stack:
  stage: Destroy
  script:
    - cd terraform/apps/wso2/config
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR" || true
    - cd $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/mojaloop-roles
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR" || true
    - cd $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR" || true
    - kubectl --kubeconfig=../../admin-gateway.conf delete namespace mojaloop || true
    ##kill force kill efs provsioner
    - kubectl --kubeconfig=../../admin-gateway.conf delete pods --all -n wso2 --grace-period=0 --force
    - kubectl --kubeconfig=../../admin-gateway.conf delete namespace wso2
  when: manual

Destroy Add-On Apps:
  stage: Destroy
  script:
    - aws s3 sync s3://${BUCKET}/${environment}/k3saddons/pm4ml-certoutput/ terraform/k8s-setup/addons/pm4ml-certoutput
    - aws s3 sync s3://${client}${environment}k3s-pm4mladdons-state/k3s ./k3sstate --sse || true
    - cp $CI_PROJECT_DIR/ext-pm4ml-certs.yaml terraform/k8s-setup/addons || true
    - for var in $(jq -r '.internal_pm4ml_configs | to_entries[] .value.DFSP_NAME' ./workbench-config.json); do touch terraform/k8s-setup/addons/pm4ml-certoutput/$var-client-key.pem terraform/k8s-setup/addons/pm4ml-certoutput/$var-ca-cert.pem terraform/k8s-setup/addons/pm4ml-certoutput/$var-client-cert.pem; done
    - cd terraform/k8s-setup/addons
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - aws s3 rm --recursive s3://$BUCKET/$environment/k8s-cluster
    - aws s3 rm s3://$BUCKET/$environment/k3saddons/onboarding_pm4ml_output.json
    - aws s3 rm s3://$BUCKET/$environment/k3saddons/ansible_pm4ml_output.yaml
  when: manual

Destroy of Internal PM4MLs Setup:
  stage: Destroy
  script:
    - cd terraform/k8s-setup/pm4mls
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - aws s3 rm s3://$BUCKET/$environment/k3saddons/ansible_internal_pm4ml_output.json
    - aws s3 rm s3://$BUCKET/$environment/k3saddons/onboarding_internal_pm4ml_output.json
    - aws s3 rm s3://$BUCKET/$environment/k3saddons/ansible_external_pm4ml_output.yaml
  when: manual

Destroy Vault:
  stage: Destroy
  script:
    - cd terraform/k8s-setup/vault-deploy
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform validate
    - terraform destroy -auto-approve -var="aws_secret_key=$AWS_SECRET_ACCESS_KEY" -var="aws_access_key=$AWS_ACCESS_KEY_ID" -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
  when: manual

Destroy Environment:
  stage: Destroy
  script:
    - aws s3 sync s3://${BUCKET}/${environment}/k3saddons/pm4ml-certoutput/ terraform/k8s-setup/addons/pm4ml-certoutput
    - aws s3 sync s3://${client}${environment}k3s-pm4mladdons-state/k3s ./k3sstate --sse || true
    - for var in $(jq -r '.internal_pm4ml_configs | to_entries[] .value.DFSP_NAME' ./workbench-config.json); do touch terraform/k8s-setup/addons/pm4ml-certoutput/$var-client-key.pem terraform/k8s-setup/addons/pm4ml-certoutput/$var-ca-cert.pem terraform/k8s-setup/addons/pm4ml-certoutput/$var-client-cert.pem; done
    - cp $CI_PROJECT_DIR/ext-pm4ml-certs.yaml terraform/k8s-setup/addons || true
    - cp $CI_PROJECT_DIR/teardown.sh $CI_IMAGE_PROJECT_DIR/teardown.sh || true
    - $CI_IMAGE_PROJECT_DIR/teardown.sh
  when: manual

Destroy k3s Cluster:
  stage: "Destroy"  
  image: 
    name: ghcr.io/pm4ml/k3sbootstrap:0.12.9
  before_script:
    - export BUCKET=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - export TF_VAR_bucket=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"')
    - aws s3 sync s3://$BUCKET/bootstrap ./k8ss3bootstrap/ --sse || true
    - export CI_IMAGE_PROJECT_DIR=/k3s-boot
    - export peer_vpc_id=$(terraform output -state=k8ss3bootstrap/terraform.tfstate vpc_id | tr -d '"')
    - for var in $(jq -r 'to_entries[] | "\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - for var in $(jq -r 'to_entries[] | "TF_VAR_\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - sed -i "s/##accesskeyid##/${AWS_ACCESS_KEY_ID}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s|##accesssecret##|${AWS_SECRET_ACCESS_KEY}|g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s/##region##/${region}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - mkdir -p ~/.aws && cp $CI_IMAGE_PROJECT_DIR/aws_conf ~/.aws/credentials
    - "which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )"
    - eval `ssh-agent`
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - if [ -d ./k3s-sync-dir ]; then cp -r ./k3s-sync-dir/. /k3s-boot; else echo "not local files to copy"; fi 
    - aws s3 sync s3://$BUCKET/$environment ./k8ss3/ --sse || true
    - export domain=$(terraform output -state=k8ss3/terraform.tfstate public_subdomain | tr -d '"')

  script:
    - export
    - cd /k3s-boot
    - sed -i "s/##client##/${client}/g" .env
    - sed -i "s/##domain##/${domain}/g" .env
    - sed -i "s/##environment##/${environment}/g" .env
    - sed -i "s/##region##/${region}/g" .env
    - make backend -- -auto-approve || true
    - make destroy -- -auto-approve || true 
    - aws s3 rm --recursive s3://$BUCKET/$environment/k3saddons || true 
    - aws s3 rm --recursive s3://${client}${environment}k3s-pm4mladdons-state/k3s || true
    - $CI_IMAGE_PROJECT_DIR/delete_int_k3s_db_row.sh ${client}${environment}k3s-pm4mladdons-lock ${client}${environment}k3s-pm4mladdons-state/k3s $region
  when: manual

Undeploy PM4ML Charts:
  stage: "Destroy"  
  image: 
    name: ghcr.io/pm4ml/k3sbootstrap:0.12.9
  before_script:
    - export BUCKET=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - export TF_VAR_bucket=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - aws s3 sync s3://$BUCKET/bootstrap ./k8ss3bootstrap/ --sse || true
    - export CI_IMAGE_PROJECT_DIR=/k3s-boot
    - export peer_vpc_id=$(terraform output -state=k8ss3bootstrap/terraform.tfstate vpc_id | tr -d '"')
    - for var in $(jq -r 'to_entries[] | "\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - for var in $(jq -r 'to_entries[] | "TF_VAR_\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - sed -i "s/##accesskeyid##/${AWS_ACCESS_KEY_ID}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s|##accesssecret##|${AWS_SECRET_ACCESS_KEY}|g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s/##region##/${region}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - mkdir -p ~/.aws && cp $CI_IMAGE_PROJECT_DIR/aws_conf ~/.aws/credentials
    - "which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )"
    - eval `ssh-agent`
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - if [ -d ./k3s-sync-dir ]; then cp -r ./k3s-sync-dir/. /k3s-boot; else echo "not local files to copy"; fi 
    - aws s3 sync s3://$BUCKET/$environment ./k8ss3/ --sse || true
    - export domain=$(terraform output -state=k8ss3/terraform.tfstate public_subdomain | tr -d '"')

  script:
    - aws s3 cp s3://$BUCKET/$environment/k3saddons/ssh-key /k3s-boot/ssh-key --sse
    - chmod 400 /k3s-boot/ssh-key
    - cd /k3s-boot
    - sed -i "s/##client##/${client}/g" .env
    - sed -i "s/##domain##/${domain}/g" .env
    - sed -i "s/##environment##/${environment}/g" .env
    - sed -i "s/##region##/${region}/g" .env
    - sed -i "s/##peer_vpc_id##/${peer_vpc_id}/g" .env
    #TODO: static value needs addressing
    - sed -i "s/##pm4ml_client_cert_local_dir##/\/k3s-boot\/pm4ml-certoutput\//g" .env
    - aws s3 cp s3://$BUCKET/$environment/k3saddons/ansible_internal_pm4ml_output.yaml ./ansible_pm4ml_output.yaml --sse
    - make backend -- -auto-approve || true
    - make apply -- -auto-approve || true
    - make uninstall-pm4ml
    - aws s3 rm --recursive s3://$BUCKET/$PM4ML_ENVIRONMENT/pm4ml-certoutput
  when: manual

Deploy AWS Infrastructure:
  stage: "Deploy AWS infrastructure"
  script:
    - cd terraform
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve
    - aws s3 sync $CI_IMAGE_PROJECT_DIR/kubespray-inventory s3://${BUCKET}/${environment}/kubespray-inventory/ --sse
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/terraform/ssh_provisioner_key s3://${BUCKET}/${environment}/terraform/ --sse
  when: manual

Create k3s Cluster:
  stage: "Run k3s"
  image: 
    name: ghcr.io/pm4ml/k3sbootstrap:0.12.9
  before_script:
    - export BUCKET=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - export TF_VAR_bucket=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - aws s3 sync s3://$BUCKET/bootstrap ./k8ss3bootstrap/ --sse || true
    - export CI_IMAGE_PROJECT_DIR=/k3s-boot
    - export peer_vpc_id=$(terraform output -state=k8ss3bootstrap/terraform.tfstate vpc_id | tr -d '"')
    - for var in $(jq -r 'to_entries[] | "\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - for var in $(jq -r 'to_entries[] | "TF_VAR_\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - sed -i "s/##accesskeyid##/${AWS_ACCESS_KEY_ID}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s|##accesssecret##|${AWS_SECRET_ACCESS_KEY}|g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s/##region##/${region}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - mkdir -p ~/.aws && cp $CI_IMAGE_PROJECT_DIR/aws_conf ~/.aws/credentials
    - "which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )"
    - eval `ssh-agent`
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - if [ -d ./k3s-sync-dir ]; then cp -r ./k3s-sync-dir/. /k3s-boot; else echo "not local files to copy"; fi 
    - aws s3 sync s3://$BUCKET/$environment ./k8ss3/ --sse || true
    - export domain=$(terraform output -state=k8ss3/terraform.tfstate public_subdomain | tr -d '"')
      
  script:
    - export
    - cd /k3s-boot
    - sed -i "s/##client##/${client}/g" .env
    - sed -i "s/##domain##/${domain}/g" .env
    - sed -i "s/##environment##/${environment}/g" .env
    - sed -i "s/##region##/${region}/g" .env
    - sed -i "s/##peer_vpc_id##/${peer_vpc_id}/g" .env
    - make backend -- -auto-approve
    - make apply -- -auto-approve
    - make k3s
    - make kubeconfig
    - aws s3 cp /k3s-boot/kubeconfig s3://$BUCKET/$environment/k3saddons/ --sse
    - aws s3 cp /k3s-boot/ssh-key s3://${BUCKET}/$environment/k3saddons/ --sse
    - aws s3 sync s3://$BUCKET/$environment/k3saddons /k3s-boot/k3saddons --sse
    - cp -r /k3s-boot/k3saddons/* /k3s-boot/
    - if [ ! -f /k3s-boot/vault-keys.json ]; then make vault; else echo "found vault, skipping make vault"; fi 
    - aws s3 cp /k3s-boot/vault-keys.json s3://$BUCKET/$environment/k3saddons/ --sse   
    - make wireguard
    - aws s3 cp /k3s-boot/wireguard.private.key s3://$BUCKET/$environment/k3saddons/ --sse
    - aws s3 cp /k3s-boot/wireguard.public.key s3://$BUCKET/$environment/k3saddons/ --sse
    - make monitoring 
  when: manual

Create Addons Cluster:
  stage: "Run Kubespray"
  script:
    - cd /kubespray
    - ansible-playbook -i $CI_IMAGE_PROJECT_DIR/kubespray-inventory/hosts-add-ons -e @$CI_IMAGE_PROJECT_DIR/kubespray-inventory/extra-vars.json ./cluster.yml $ANSIBLE_FEATURE_TOGGLES
    - aws s3 cp inventory/artifacts/cluster/admin.conf s3://$BUCKET/$environment/admin-add-ons.conf --sse
  when: manual

Create Gateway Cluster:
  stage: "Run Kubespray"
  script:
    - cd /kubespray
    - ansible-playbook -i $CI_IMAGE_PROJECT_DIR/kubespray-inventory/hosts-gateway -e @$CI_IMAGE_PROJECT_DIR/kubespray-inventory/extra-vars.json ./cluster.yml $ANSIBLE_FEATURE_TOGGLES
    - aws s3 cp inventory/artifacts/cluster/admin.conf s3://$BUCKET/$environment/admin-gateway.conf --sse
  when: manual

Deploy Vault:
  stage: "Deploy Vault"
  script:
    - cd terraform/k8s-setup/vault-deploy
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="aws_secret_key=$AWS_SECRET_ACCESS_KEY" -var="aws_access_key=$AWS_ACCESS_KEY_ID" -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/vault_seal_key s3://${BUCKET}/${environment}/ --sse
  when: manual

1. Initial Install Base Platform:
  stage: "Deploy Platform"
  script:
    - cd terraform/k8s-setup
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - cd $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/mojaloop-roles
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - chmod o-w $CI_IMAGE_PROJECT_DIR/terraform/apps/wso2/config/
    - cd $CI_IMAGE_PROJECT_DIR/terraform/apps/wso2/config/
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - ansible-playbook publish.yaml
  when: manual

2a. Setup PM4MLs:
  stage: "Deploy Platform"
  script:
    - cd terraform/k8s-setup/pm4mls
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/pm4mls/ansible_internal_pm4ml_output.yaml s3://${BUCKET}/${environment}/k3saddons/ --sse
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/pm4mls/onboarding_internal_pm4ml_output.json s3://${BUCKET}/${environment}/k3saddons/ --sse
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/pm4mls/ansible_external_pm4ml_output.yaml s3://${BUCKET}/${environment}/k3saddons/ --sse
    - aws s3 cp $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/pm4mls/onboarding_external_pm4ml_output.json s3://${BUCKET}/${environment}/k3saddons/ --sse
    - aws s3 sync $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/pm4mls/secrets_chart/ s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart/ --sse
  when: manual

2b. Install Internal PM4MLs:
  stage: "Deploy Platform"
  image: 
    name: ghcr.io/pm4ml/k3sbootstrap:0.12.9
  before_script:
    - export BUCKET=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - export TF_VAR_bucket=$(grep bucket backend.hcl | cut -f2 -d '=' | tr -d '"' | xargs)
    - aws s3 sync s3://$BUCKET/bootstrap ./k8ss3bootstrap/ --sse || true
    - export CI_IMAGE_PROJECT_DIR=/k3s-boot
    - export peer_vpc_id=$(terraform output -state=k8ss3bootstrap/terraform.tfstate vpc_id | tr -d '"')
    - for var in $(jq -r 'to_entries[] | "\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - for var in $(jq -r 'to_entries[] | "TF_VAR_\(.key)=\(.value)\n"' ./workbench-config.json); do export $var; done
    - sed -i "s/##accesskeyid##/${AWS_ACCESS_KEY_ID}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s|##accesssecret##|${AWS_SECRET_ACCESS_KEY}|g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - sed -i "s/##region##/${region}/g" $CI_IMAGE_PROJECT_DIR/aws_conf
    - mkdir -p ~/.aws && cp $CI_IMAGE_PROJECT_DIR/aws_conf ~/.aws/credentials
    - "which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )"
    - eval `ssh-agent`
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - if [ -d ./k3s-sync-dir ]; then cp -r ./k3s-sync-dir/. /k3s-boot; else echo "not local files to copy"; fi 
    - aws s3 sync s3://$BUCKET/$environment ./k8ss3/ --sse || true
    - export domain=$(terraform output -state=k8ss3/terraform.tfstate public_subdomain | tr -d '"')
  script:
    - aws s3 cp s3://$BUCKET/$environment/k3saddons/ssh-key /k3s-boot/ssh-key --sse
    - chmod 400 /k3s-boot/ssh-key
    - cd /k3s-boot
    - sed -i "s/##client##/${client}/g" .env
    - sed -i "s/##domain##/${domain}/g" .env
    - sed -i "s/##environment##/${environment}/g" .env
    - sed -i "s/##region##/${region}/g" .env
    - sed -i "s/##peer_vpc_id##/${peer_vpc_id}/g" .env
    #TODO: static value needs addressing
    - sed -i "s/##pm4ml_client_cert_local_dir##/\/k3s-boot\/pm4ml-certoutput\//g" .env
    - aws s3 cp s3://$BUCKET/$environment/k3saddons/ansible_internal_pm4ml_output.yaml ./ansible_pm4ml_output.yaml --sse
    - make backend -- -auto-approve
    - make apply -- -auto-approve
    - make pm4ml
    - aws s3 sync /k3s-boot/pm4ml-certoutput s3://$BUCKET/$environment/k3saddons/pm4ml-certoutput --sse
  when: manual

2c. Install Post Deployment:
  stage: "Deploy Platform"
  script:
  #todo remove hardcoded values here
    - aws s3 sync s3://${BUCKET}/${environment}/k3saddons/pm4ml-certoutput/ terraform/k8s-setup/addons/pm4ml-certoutput
    - aws s3 sync s3://${client}${environment}k3s-pm4mladdons-state/k3s ./k3sstate --sse || true
    - if [ -f k3sstate/terraform.tfstate ]; then export pm4ml_nat_ips_tmp=$(terraform output -state=k3sstate/terraform.tfstate nat_public_ips); else ls -la ./k3sstate; fi 
    - echo $pm4ml_nat_ips_tmp
    - if [ $(echo $pm4ml_nat_ips_tmp | grep --quiet Warning) ]; then echo "nats not found"; else export TF_VAR_pm4ml_nat_ips=$pm4ml_nat_ips_tmp; fi
    - cp $CI_PROJECT_DIR/ext-pm4ml-certs.yaml terraform/k8s-setup/addons || true
    - cd terraform/k8s-setup/addons
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
    - aws s3 sync $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/addons/secrets_chart s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart --sse
    - aws s3 sync $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/addons/sim_tests s3://${BUCKET}/${environment}/k8s-cluster/sim_tests --sse
    - find . -type f -iname '*_results' -exec cat '{}' \;
  when: manual


#job template for 3. Run ML GP Tests	  
.Run ML GP Tests-job-template:
  script:
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/sim_tests/ $CI_IMAGE_PROJECT_DIR/sim_tests --sse
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart/ $CI_IMAGE_PROJECT_DIR/secrets_chart --sse
    - cd sim_tests
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Deregister_Existing_Oracles.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopHub_Setup.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopSims_Onboarding.postman_collection.json --ssl-client-cert-list test_cert_list.json --insecure -e Lab.postman_environment.json || true
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/Golden_Path_Mojaloop.postman_collection.json --ssl-client-cert-list test_cert_list.json --insecure -e Lab.postman_environment.json --folder p2p_happy_path_RECEIVE_quote --reporters html,junit,cli --reporter-html-export ${CI_PROJECT_DIR}/hp_report.html --reporter-junit-export ${CI_PROJECT_DIR}/hp_report.xml --reporter-html-template ../tests/template-default-colored.hbs -x
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/Golden_Path_Mojaloop.postman_collection.json --ssl-client-cert-list test_cert_list.json --insecure -e Lab.postman_environment.json --reporters html,junit,cli --reporter-html-export ${CI_PROJECT_DIR}/gp_report.html --reporter-junit-export ${CI_PROJECT_DIR}/gp_report.xml --reporter-html-template ../tests/template-default-colored.hbs -x
     
  after_script:
    - curl https://slack.com/api/files.upload
      -F file=@"gp_report.html"
      -F channels="${SLACK_GITLABCI_ALERTS_CHANNEL_NAME}","${SLACK_GITLABCI_GP_REPORTS_CHANNEL_NAME}"
      -F token="${SLACK_GITLABCI_ALERTS_TOKEN}"
      -F title="Mojaloop Goldenpath Test (Postman) Results.'${ENVIRONMENT}'-$(date +\"%Y-%m-%d-%H:%M:%S\")"
      -F filetype="html"
  artifacts:
    paths:
      - gp_report.html
    reports:
      junit: gp_report.xml

1. Manual Run ML GP Tests:
  stage: "Run Tests"
  allow_failure: true
  extends: .Run ML GP Tests-job-template
  when: manual
  except:
    - schedules

1. Scheduled Run ML GP Tests:
  stage: "Run Tests"
  allow_failure: true
  extends: .Run ML GP Tests-job-template
  only:
    refs:
      - schedules
    variables:
      - $SCHEDULE_GP == "1_GP"
      

#job template for 2. Run PM4ML GP Tests	  
.Run PM4ML GP Tests-job-template:
  script:
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/sim_tests/ $CI_IMAGE_PROJECT_DIR/sim_tests --sse
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart/ $CI_IMAGE_PROJECT_DIR/secrets_chart --sse
    - aws s3 cp s3://${BUCKET}/${environment}/k3saddons/onboarding_internal_pm4ml_output.json $CI_IMAGE_PROJECT_DIR/sim_tests
    - cd sim_tests
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Deregister_Existing_Oracles.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopHub_Setup.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopSims_Onboarding.postman_collection.json --ssl-client-cert-list test_cert_list.json --insecure -e Lab.postman_environment.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Onboard-Generic-FSP-Central_Ledger.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json -d onboarding_internal_pm4ml_output.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Onboard-Generic-FSP-Sim_Backend.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json -d onboarding_internal_pm4ml_output.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Mojaloop-Payment-Manager-Golden-Path.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json --folder feature-tests --reporters html,junit,cli --reporter-html-export ${CI_PROJECT_DIR}/gp_report.html --reporter-junit-export ${CI_PROJECT_DIR}/gp_report.xml --reporter-html-template ../tests/template-default-colored.hbs -x
  after_script:
    - curl https://slack.com/api/files.upload
      -F file=@"gp_report.html"
      -F channels="${SLACK_GITLABCI_ALERTS_CHANNEL_NAME}","${SLACK_GITLABCI_GP_REPORTS_CHANNEL_NAME}"
      -F token="${SLACK_GITLABCI_ALERTS_TOKEN}"
      -F title="Mojaloop PM4ML Goldenpath Test (Postman) Results.'${ENVIRONMENT}'-$(date +\"%Y-%m-%d-%H:%M:%S\")"
      -F filetype="html"
  artifacts:
    paths:
      - gp_report.html
    reports:
      junit: gp_report.xml

2. Manual Run PM4ML GP Tests:
  stage: "Run Tests"
  extends: .Run PM4ML GP Tests-job-template
  when: manual
  except:
    - schedules

2. Scheduled Run PM4ML GP Tests:
  stage: "Run Tests"
  allow_failure: true
  extends: .Run PM4ML GP Tests-job-template
  only:
    refs:
      - schedules
    variables:
      - $SCHEDULE_PM4ML == "PM4ML_GP"

3. Run Finance Portal V2 UI Tests:
  stage: "Run Tests"
  allow_failure: true
  when: manual
  script:
    - if [ -f terraform-k8s.tfstate ]; then export fin_portal_users=$(terraform output -json -state=terraform-k8s.tfstate finance_portal_users); else ls -la .; fi 
    - if [ -f terraform-k8s.tfstate ]; then export fin_portal_url=$(terraform output -state=terraform-k8s.tfstate finance-portal-url); else ls -la .; fi 
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/sim_tests/ ${CI_IMAGE_PROJECT_DIR}/sim_tests --sse
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart/ ${CI_IMAGE_PROJECT_DIR}/secrets_chart --sse
    - aws s3 cp s3://${BUCKET}/${environment}/k3saddons/onboarding_internal_pm4ml_output.json ${CI_IMAGE_PROJECT_DIR}/sim_tests
    - cd ${CI_IMAGE_PROJECT_DIR}/sim_tests
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Deregister_Existing_Oracles.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopHub_Setup.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/mojaloop/postman/${TAG_ML_GP}/MojaloopSims_Onboarding.postman_collection.json --ssl-client-cert-list test_cert_list.json --insecure -e Lab.postman_environment.json || true
    - git clone https://github.com/mojaloop/finance-portal-v2-ui.git ${CI_IMAGE_PROJECT_DIR}/finance-portal-v2-ui
    - cd ${CI_IMAGE_PROJECT_DIR}/finance-portal-v2-ui/tests/e2e-ui-tests
    - echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list
    - wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add -
    - apt-get update
    - apt-get install -y google-chrome-stable
    - echo $fin_portal_users | jq -r '"ADMIN_USER_NAME=\(.portaladmin.username)\nADMIN_PASSWORD=\(.portaladmin.user_pass)\nUSER_NAME=\(.portaluser.username)\nPASSWORD=\(.portaluser.user_pass)"' > .env
    - echo "FINANCE_PORTAL_ENDPOINT=$fin_portal_url" >> .env
    - npm i
    - npm run test:headless || true
    - cp ${CI_IMAGE_PROJECT_DIR}/finance-portal-v2-ui/tests/e2e-ui-tests/src/reports/report.html ${CI_PROJECT_DIR}/fin_port_report.html
  after_script:
    - curl https://slack.com/api/files.upload
      -F file=@"${CI_IMAGE_PROJECT_DIR}/finance-portal-v2-ui/tests/e2e-ui-tests/src/reports/report.html"
      -F channels="${SLACK_GITLABCI_ALERTS_CHANNEL_NAME}"
      -F token="${SLACK_GITLABCI_ALERTS_TOKEN}"
      -F title="Run Finance Portal V2 UI Tests"
      -F filetype="html"
  artifacts:
    paths:
      - fin_port_report.html
    #reports:
      #junit: $CI_IMAGE_PROJECT_DIR/finance-portal-v2-ui/tests/e2e-ui-tests/src/reports/report.html

#job template for 4. Run Platform GP Tests:
.Run Platform GP Tests-job-template:
  script:
    - aws s3 sync s3://${BUCKET}/${TF_VAR_environment}/k8s-cluster/sim_tests/ $CI_IMAGE_PROJECT_DIR/sim_tests --sse
    - aws s3 sync s3://${BUCKET}/${TF_VAR_environment}/k8s-cluster/secrets_chart/ $CI_IMAGE_PROJECT_DIR/secrets_chart --sse
    - aws s3 cp s3://${BUCKET}/${TF_VAR_environment}/k3saddons/onboarding_internal_pm4ml_output.json $CI_IMAGE_PROJECT_DIR/sim_tests
    - aws s3 cp s3://${BUCKET}/${TF_VAR_environment}/k3saddons/onboarding_external_pm4ml_output.json $CI_IMAGE_PROJECT_DIR/sim_tests
    - cp $CI_IMAGE_PROJECT_DIR/tests/Golden-Path.postman_collection.json $CI_IMAGE_PROJECT_DIR/sim_tests
    - cd sim_tests
    - jq -s '[.[][]]' onboarding_external_pm4ml_output.json onboarding_internal_pm4ml_output.json > /tmp/onboarding_pm4ml_output.json
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Deregister_Existing_Oracles.postman_collection.json --insecure -e Lab.postman_environment.json
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Onboard-Generic-FSP-Central_Ledger.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json -d /tmp/onboarding_pm4ml_output.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Account_Alias_Oracle_Registration.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Account_Alias_Oracle_Setup.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json -d /tmp/onboarding_pm4ml_output.json || true
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Onboard-Generic-FSP-Sim_Backend.postman_collection.json --insecure -e Lab.postman_environment.json --ssl-client-cert-list test_cert_list.json -d onboarding_internal_pm4ml_output.json || true
    - newman run Golden-Path.postman_collection.json --ssl-client-cert-list test_cert_list.json --folder loan-repayment --insecure -e Lab.postman_environment.json --reporters html,junit,cli --reporter-html-export ${CI_PROJECT_DIR}/alias_report.html --reporter-junit-export ${CI_PROJECT_DIR}/alias_report.xml --reporter-html-template ../tests/template-default-colored.hbs -x
  after_script:
#    - curl https://slack.com/api/files.upload
#      -F file=@"Platform_report.html"
#      -F channels="${SLACK_GITLABCI_ALERTS_CHANNEL_NAME}"
#      -F token="${SLACK_GITLABCI_ALERTS_TOKEN}"
#      -F title="Myanmar Goldenpath Test (Postman) Results"
#      -F filetype="html"
#  artifacts:
#    reports:
#      junit: alias_report.xml
      
    - curl https://slack.com/api/files.upload
      -F file=@"alias_report.html"
      -F channels="${SLACK_GITLABCI_GP_REPORTS_CHANNEL_NAME}"
      -F token="${SLACK_GITLABCI_ALERTS_TOKEN}"
      -F title="Alias Goldenpath Test (Postman) Results.'${ENVIRONMENT}'-$(date +\"%Y-%m-%d-%H:%M:%S\")"
      -F filetype="html"
  artifacts:
    reports:
      junit: alias_report.xml


4. Manual Run Platform GP Tests:
  stage: "Run Tests"
  extends: .Run Platform GP Tests-job-template
  when: manual
  except:
    - schedules

4. Scheduled Run Platform GP Tests:
  stage: "Run Tests"
  allow_failure: true
  extends: .Run Platform GP Tests-job-template
  only:
    refs:
      - schedules
    variables:
      - $SCHEDULE_PLATFORM == "PLAT_GP"

5. Run Helm Tests:
  stage: "Run Tests"
  script:
    - helm --kubeconfig=admin-gateway.conf test ${helm_mojaloop_release_name} -n mojaloop
  when: manual

1. Run External PM4ML Onboarding:
  stage: "Maintain Platform"
  script:
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/sim_tests/ $CI_IMAGE_PROJECT_DIR/sim_tests --sse
    - aws s3 sync s3://${BUCKET}/${environment}/k8s-cluster/secrets_chart/ $CI_IMAGE_PROJECT_DIR/secrets_chart --sse
    - aws s3 cp s3://${BUCKET}/${environment}/k3saddons/onboarding_external_pm4ml_output.json $CI_IMAGE_PROJECT_DIR/sim_tests
    - cd sim_tests
    - newman run https://raw.githubusercontent.com/pm4ml/pm4ml-test-scripts/${TAG_PM4ML_GP}/postman/Onboard-Generic-FSP-Central_Ledger.postman_collection.json --insecure -e Lab.postman_environment.json -d onboarding_external_pm4ml_output.json
  when: manual

2. Update oathkeeper rules:
  stage: "Maintain Platform"
  script:
    - export private_subdomain=$(terraform output -state=terraform.tfstate private_subdomain | tr -d '"')
    - helm -n mojaloop -i --kubeconfig=admin-gateway.conf --set-string base_domain=${private_subdomain} --set-string bof_release_name=bof upgrade bof-oathkeeper-rules $CI_IMAGE_PROJECT_DIR/bof-custom-resources/oathkeeper-rules
  when: manual

3. Update mojaloop roles:
  stage: "Maintain Platform"
  script:
    - cd $CI_IMAGE_PROJECT_DIR/terraform/k8s-setup/mojaloop-roles
    - terraform init -backend-config ${CI_PROJECT_DIR}/backend.hcl
    - terraform apply -auto-approve -var="project_root_path=$CI_IMAGE_PROJECT_DIR"
  when: manual